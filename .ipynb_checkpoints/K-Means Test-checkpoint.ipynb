{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150L, 4L)\n",
      "(150L,)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "[0 1 2]\n",
      "n_digits: 3, \t n_samples 150, \t n_features 4\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print iris.data.shape\n",
    "print iris.target.shape\n",
    "\n",
    "data = scale(iris.data)\n",
    "\n",
    "n_samples, n_features = data.shape\n",
    "n_digits = len(np.unique(iris.target))\n",
    "print iris.target\n",
    "\n",
    "print np.unique(iris.target)\n",
    "labels = iris.target\n",
    "\n",
    "sample_size = 300\n",
    "\n",
    "print(\"n_digits: %d, \\t n_samples %d, \\t n_features %d\"\n",
    "      % (n_digits, n_samples, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf=svm.LinearSVC()\n",
    "# learn from existing data using estimator called fit(x,y)\n",
    "clf.fit(iris.data,iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.18423281,  0.45122677, -0.80794335, -0.45071701],\n",
       "       [ 0.05182102, -0.89410543,  0.40512989, -0.93782962],\n",
       "       [-0.85076016, -0.98680704,  1.38094499,  1.86551338]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use model to predict most likely outcome on unseen data\n",
    "\n",
    "clf.predict([[5.0,3.6,1.3,0.25]])\n",
    "clf.coef_ #access parameters of model via attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classification:\n",
    "\n",
    "from sklearn import neighbors\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "knn.fit(iris.data,iris.target)\n",
    "\n",
    "knn.predict([[0.1,0.2,0.3,0.4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95999999999999996"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm = np.random.permutation(iris.target.size)\n",
    "iris.data = iris.data[perm]\n",
    "iris.target = iris.target[perm]\n",
    "knn.fit(iris.data[:100],iris.target[:100])\n",
    "\n",
    "knn.score(iris.data[100:],iris.target[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]\n",
      "[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "#K-mean clustering:\n",
    "\n",
    "from sklearn import cluster,datasets\n",
    "iris = datasets.load_iris()\n",
    "k_means = cluster.KMeans(3) #three parameters\n",
    "k_means.fit(iris.data)\n",
    "\n",
    "print k_means.labels_[::10]\n",
    "print iris.target[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________________________________________________\n",
      "init    time  inertia    homo   compl  v-meas     ARI AMI  silhouette\n",
      "k-means++   0.02s    141   0.641   0.644   0.643   0.592   0.637    0.462\n",
      "   random   0.03s    141   0.641   0.644   0.643   0.592   0.637    0.462\n",
      "PCA-based   0.00s    142   0.660   0.662   0.661   0.645   0.656    0.455\n",
      "_______________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(79 * '_')\n",
    "print('% 9s' % 'init'\n",
    "      '    time  inertia    homo   compl  v-meas     ARI AMI  silhouette')\n",
    "\n",
    "\n",
    "def bench_k_means(estimator, name, data):\n",
    "    t0 = time()\n",
    "    estimator.fit(data)\n",
    "    print('% 9s   %.2fs    %i   %.3f   %.3f   %.3f   %.3f   %.3f    %.3f'\n",
    "          % (name, (time() - t0), estimator.inertia_,\n",
    "             metrics.homogeneity_score(labels, estimator.labels_),\n",
    "             metrics.completeness_score(labels, estimator.labels_),\n",
    "             metrics.v_measure_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n",
    "             metrics.silhouette_score(data, estimator.labels_,\n",
    "                                      metric='euclidean',\n",
    "                                      sample_size=sample_size)))\n",
    "\n",
    "bench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),\n",
    "              name=\"k-means++\", data=data)\n",
    "\n",
    "bench_k_means(KMeans(init='random', n_clusters=n_digits, n_init=10),\n",
    "              name=\"random\", data=data)\n",
    "\n",
    "# in this case the seeding of the centers is deterministic, hence we run the\n",
    "# kmeans algorithm only once with n_init=1\n",
    "pca = PCA(n_components=n_digits).fit(data)\n",
    "bench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),\n",
    "              name=\"PCA-based\",\n",
    "              data=data)\n",
    "print(79 * '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(565L,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f37737b89392>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mlfw_people\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch_lfw_people\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_faces_per_person\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m70\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import cross_validation, datasets, decomposition, svm\n",
    "import glob\n",
    "%matplotlib inline\n",
    "# ..\n",
    "# .. load data ..\n",
    "\n",
    "\n",
    "spiders = glob.glob('./Spider/*')\n",
    "# print spiders\n",
    "fnlist = []\n",
    "fnlist[0:len(spiders)-1] = spiders[:]\n",
    "print np.shape(fnlist)\n",
    "# print spiders[:]\n",
    "\n",
    "img = []\n",
    "for i in range(0,len(spiders)):\n",
    "    img.append(mpimg.imread(spiders[i]))\n",
    "    \n",
    "\n",
    "from sklearn import cluster,datasets\n",
    "n_samples = len(img)\n",
    "for i in img:\n",
    "    data = i.reshape((n_samples, -1))\n",
    "\n",
    "lfw_people = datasets.fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "faces = np.reshape(lfw_people.data, (lfw_people.target.shape[0], -1))\n",
    "train, test = iter(cross_validation.StratifiedKFold(lfw_people.target, k=4)).next()\n",
    "X_train, X_test = faces[train], faces[test]\n",
    "y_train, y_test = lfw_people.target[train], lfw_people.target[test]\n",
    "\n",
    "# ..\n",
    "# .. dimension reduction ..\n",
    "# pca = decomposition.RandomizedPCA(n_components=150, whiten=True)\n",
    "# pca.fit(X_train)\n",
    "# X_train_pca = pca.transform(X_train)\n",
    "# X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# # ..\n",
    "# # .. classification ..\n",
    "# clf = svm.SVC(C=5., gamma=0.001)\n",
    "# clf.fit(X_train_pca, y_train)\n",
    "\n",
    "# print 'Score on unseen data: '\n",
    "# print clf.score(X_test_pca, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (400,400,3) into shape (400,400)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-64e90eabb05a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mtestLabs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mtestImgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestImgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[0mTwoDim_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestImgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestSize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (400,400,3) into shape (400,400)"
     ]
    }
   ],
   "source": [
    "import  sklearn as sk\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "spiders = glob.glob('./Spider/*')\n",
    "non_spiders = glob.glob('./NonSpider/*')\n",
    "\n",
    "SPimg = []\n",
    "for sp in spiders:\n",
    "    img = Image.open(sp)\n",
    "    resized = scipy.misc.imresize(img,[400,400])\n",
    "    SPimg.append(resized)\n",
    "\n",
    "NSPimg = []\n",
    "for sp in non_spiders:\n",
    "    img = Image.open(sp)\n",
    "    resized = scipy.misc.imresize(img,[400,400])\n",
    "    NSPimg.append(resized)\n",
    "\n",
    "testSize = int(0.75*len(SPimg)) + int(0.75*len(NSPimg))\n",
    "checkSpid = int(0.25*len(SPimg))\n",
    "checkNSpid = int(0.25*len(NSPimg))\n",
    "testLabs = np.zeros(testSize)\n",
    "\n",
    "testImgs = SPimg[0:int(0.75*len(SPimg))]\n",
    "\n",
    "for i in range(0,int(0.75*len(SPimg))):\n",
    "    testLabs[i] = 1\n",
    "# testLabs = np.ones(int(0.75*len(SPimg)))\n",
    "\n",
    "testImgs[len(testImgs):(len(testImgs)+(int)(0.75*len(NSPimg)))] = SPimg[0:(int)(0.75*len(NSPimg))]\n",
    "toAloc = -1*np.ones(int(0.75*len(NSPimg)))\n",
    "\n",
    "for i in range(int(0.75*len(NSPimg)),(len(testImgs)-2)):\n",
    "    testLabs[i] = -1\n",
    "\n",
    "testImgs = np.array(testImgs)\n",
    "TwoDim_dataset = testImgs.reshape(testSize,-1)\n",
    "\n",
    "#\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "knn.fit(TwoDim_dataset,testLabs)\n",
    "\n",
    "\n",
    "# print testSP\n",
    "# blah=[]\n",
    "\n",
    "# for i in testSP:\n",
    "#     blah1=[]\n",
    "#     for j in i:\n",
    "#         blah1.append(j)\n",
    "#     blah.append(blah1)\n",
    "# print testSP\n",
    "# print blah\n",
    "\n",
    "# blah = np.array(blah)\n",
    "print len(SPimg)\n",
    "# testSP = SPimg[int(0.75*len(SPimg)):230]\n",
    "testSP = NSPimg[int(0.75*len(NSPimg)):len(NSPimg)]\n",
    "print 0.75*len(SPimg)\n",
    "check = np.array(testSP)\n",
    "\n",
    "TwoDtestSP=check.reshape(len(NSPimg)-int(0.75*len(NSPimg)),-1)\n",
    "\n",
    "knn.predict(TwoDtestSP)\n",
    "\n",
    "# knn.score(TwoDtestSP)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# np.squeeze(x, axis=(2,)).shape\n",
    "# (1, 3)\n",
    "# check=np.array(testSP)\n",
    "# check=[item for sublist in x for item in sublist]\n",
    "# # TwoDtestSP = check.reshape(checkSpid,-1)\n",
    "\n",
    "# knn.predict(check)\n",
    "#Now for a single layer perceptron with 3 nodes in the hidden layer\n",
    "# SLPClass = MLPClassifier(hidden_layer_sizes=(3,))\n",
    "# SLPClass.fit(TwoDim_dataset,testLabs)\n",
    "\n",
    "# print SLPClass.loss_\n",
    "# testSP = np.array(SPimg[(int)(0.75*len(SPimg))+1:-1])\n",
    "# testSP = testSP.reshape(checkSpid,-1)\n",
    "# testSLPSPScore = SLPClass.score(testSP,np.ones(len(testSP)))\n",
    "# print \"Testing the SLP on spiders \" , testSLPSPScore\n",
    "#\n",
    "# testNSP = NSPimg[(int)(0.75*len(NSPimg))+1:-1]\n",
    "#\n",
    "# # testNSLPSPScore = SLPClass.score(testNSP,np.ones(len(testNSP)))\n",
    "# print \"Testing the SLP on non-spiders \" , testNSLPSPScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn.predict_proba(TwoDtestSP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(SPimg[int(213.5)+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
